{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1898721,"sourceType":"datasetVersion","datasetId":1131493},{"sourceId":5552662,"sourceType":"datasetVersion","datasetId":3198793},{"sourceId":14663127,"sourceType":"datasetVersion","datasetId":9367436},{"sourceId":735204,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":560442,"modelId":573029}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#installing dependencies\n!pip install -q transformers datasets peft bitsandbytes accelerate\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:05:49.420255Z","iopub.execute_input":"2026-01-30T07:05:49.420974Z","iopub.status.idle":"2026-01-30T07:05:55.501475Z","shell.execute_reply.started":"2026-01-30T07:05:49.420942Z","shell.execute_reply":"2026-01-30T07:05:55.500724Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"#imports\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\nfrom peft import LoraConfig, get_peft_model\nfrom datasets import Dataset\nfrom peft import PeftModel\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:06:00.675785Z","iopub.execute_input":"2026-01-30T07:06:00.676512Z","iopub.status.idle":"2026-01-30T07:06:27.772683Z","shell.execute_reply.started":"2026-01-30T07:06:00.676480Z","shell.execute_reply":"2026-01-30T07:06:27.771917Z"}},"outputs":[{"name":"stderr","text":"2026-01-30 07:06:12.580786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1769756772.722681      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1769756772.762272      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1769756773.083140      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769756773.083177      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769756773.083179      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1769756773.083182      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"#Loading the model \nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    load_in_4bit=True,\n    device_map=\"auto\"\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:06:44.553504Z","iopub.execute_input":"2026-01-30T07:06:44.554475Z","iopub.status.idle":"2026-01-30T07:06:59.532623Z","shell.execute_reply.started":"2026-01-30T07:06:44.554444Z","shell.execute_reply":"2026-01-30T07:06:59.531868Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5d625e359949a3b7d03d0791287e56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"714600502f8a4c94a1d58e1e4c3cd843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd88421d4654442bb5a662c6853040c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"050bd64d4aee422fac097fe1f11aa5bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b8001b323fb4b73bdeb622941e56393"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"689434ea6b474c85a6039f7a2305ad7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"820b424fe3554eb39bd36291239dcac8"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"#lora-config for fine-tuning\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:08:40.791084Z","iopub.execute_input":"2026-01-30T07:08:40.791927Z","iopub.status.idle":"2026-01-30T07:08:41.248632Z","shell.execute_reply.started":"2026-01-30T07:08:40.791896Z","shell.execute_reply":"2026-01-30T07:08:41.247935Z"}},"outputs":[{"name":"stdout","text":"trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.1023\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#importing the datasets for finetuning\nimport pandas as pd\ndf = pd.read_csv(\"/kaggle/input/imdb-movies-dataset/imdb_movies.csv\")\ndf.head()\n\n\ndata = []\n\nfor _, row in df.iterrows():\n    title = row[\"names\"]\n    overview = row[\"overview\"]\n    genre = row[\"genre\"]\n    score = row[\"score\"]\n\n    # Q1: plot explanation\n    q1 = f\"What is the movie {title} about?\"\n    a1 = overview\n\n    data.append({\n        \"text\": f\"### Question: {q1}\\n### Answer: {a1}\"\n    })\n\n    # Q2: genre-based recommendation\n    q2 = f\"Recommend a {genre} movie.\"\n    a2 = f\"{title} is a good recommendation. It has a user score of {score}. {overview}\"\n\n    data.append({\n        \"text\": f\"### Question: {q2}\\n### Answer: {a2}\"\n    })\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:08:45.557325Z","iopub.execute_input":"2026-01-30T07:08:45.557901Z","iopub.status.idle":"2026-01-30T07:08:46.119790Z","shell.execute_reply.started":"2026-01-30T07:08:45.557872Z","shell.execute_reply":"2026-01-30T07:08:46.119220Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#converting datasets into HuggingFace datasets \nfrom datasets import Dataset\n\n# convert list to dataset first if not already\nfull_dataset = Dataset.from_list(data)\n\n# shuffle + take subset\ndataset = full_dataset.shuffle(seed=42).select(range(6000))\n\nlen(dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:08:47.208474Z","iopub.execute_input":"2026-01-30T07:08:47.209133Z","iopub.status.idle":"2026-01-30T07:08:47.288497Z","shell.execute_reply.started":"2026-01-30T07:08:47.209102Z","shell.execute_reply":"2026-01-30T07:08:47.287602Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"6000"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def tokenize(example):\n    tokens = tokenizer(\n        example[\"text\"],\n        truncation=True,\n        padding=\"max_length\",\n        max_length=256\n    )\n    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n    return tokens\n\ndataset = dataset.map(tokenize, batched=False)\ndataset = dataset.remove_columns([\"text\"])\ndataset.set_format(\"torch\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:08:48.488635Z","iopub.execute_input":"2026-01-30T07:08:48.488920Z","iopub.status.idle":"2026-01-30T07:08:51.745113Z","shell.execute_reply.started":"2026-01-30T07:08:48.488894Z","shell.execute_reply":"2026-01-30T07:08:51.744522Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8533daa7af04a7aa24864e8a83b5515"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"#Training args \ntraining_args = TrainingArguments(\n    output_dir=\"./movie-llm\",\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4,\n    num_train_epochs=3,\n    fp16=True,\n\n    logging_strategy=\"steps\",   # üëà force logging\n    logging_steps=1,            # üëà every step\n    logging_first_step=True,\n\n    disable_tqdm=True,          # üëà KILLS progress bar (important)\n    report_to=\"none\",           # üëà no wandb / tensorboard\n    save_strategy=\"no\",\n    log_level=\"info\"\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Fine tuning the model \ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset\n)\nprint(\"üöÄ TRAINING STARTED\")\ntrainer.train()\nprint(\"‚úÖ TRAINING FINISHED\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#saving the finetuned model \ntrainer.model.save_pretrained(\"movie-lora-adapter\")\ntokenizer.save_pretrained(\"movie-lora-adapter\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After saving the finetuned model we are loading that model as LORA_PATH and testing it.","metadata":{}},{"cell_type":"code","source":"!ls /kaggle/input/d/jeevaaaa/movie-lora/movie-lora-adapter\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!unzip /kaggle/input/movie-lora-adapter/movie-lora-adapter.zip\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls movie-lora-adapter\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#using the finetuned model \nLORA_PATH = \"Jeevaaaa/movie-lora-adapter\"\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\nmodel = PeftModel.from_pretrained(\n    model,\n    LORA_PATH\n)\n\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:08:59.847385Z","iopub.execute_input":"2026-01-30T07:08:59.848004Z","iopub.status.idle":"2026-01-30T07:09:01.182684Z","shell.execute_reply.started":"2026-01-30T07:08:59.847973Z","shell.execute_reply":"2026-01-30T07:09:01.181949Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/868 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2d6e351ec6a45c3af6e35d337263a36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/4.52M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc9dba079f1a4f43826cd6ce5d3bfc5c"}},"metadata":{}},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): PeftModelForCausalLM(\n      (base_model): LoraModel(\n        (model): LlamaForCausalLM(\n          (model): LlamaModel(\n            (embed_tokens): Embedding(32000, 2048)\n            (layers): ModuleList(\n              (0-21): 22 x LlamaDecoderLayer(\n                (self_attn): LlamaAttention(\n                  (q_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2048, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=2048, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n                  (v_proj): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=2048, out_features=8, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=8, out_features=256, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                )\n                (mlp): LlamaMLP(\n                  (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n                  (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n                  (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n                  (act_fn): SiLUActivation()\n                )\n                (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n                (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n              )\n            )\n            (norm): LlamaRMSNorm((2048,), eps=1e-05)\n            (rotary_emb): LlamaRotaryEmbedding()\n          )\n          (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"#testing the finetuned model \nprompt = \"\"\"### Question: Recommend a science fiction movie with a twist.\n### Answer:\"\"\"\n\ninputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\nwith torch.no_grad():\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=120,\n        do_sample=True,          # üëà IMPORTANT\n        temperature=0.4,         # low = controlled\n        top_p=0.9,\n        repetition_penalty=1.1,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T07:09:05.389034Z","iopub.execute_input":"2026-01-30T07:09:05.389338Z","iopub.status.idle":"2026-01-30T07:09:13.704339Z","shell.execute_reply.started":"2026-01-30T07:09:05.389311Z","shell.execute_reply":"2026-01-30T07:09:13.703583Z"}},"outputs":[{"name":"stdout","text":"### Question: Recommend a science fiction movie with a twist.\n### Answer: \"The Martians\" (2017)\n\n\"The Martians\" is a science fiction movie that tells the story of a group of astronauts who land on Mars and discover that it has been colonized by an alien race, led by a mysterious and powerful being known as the Martian King. The film follows their attempts to establish contact and communication with the Martians, but they soon realize that they are not alone on the planet.\n\nThe twist in this movie is that the Martians are not hostile or aggressive, but instead,\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}